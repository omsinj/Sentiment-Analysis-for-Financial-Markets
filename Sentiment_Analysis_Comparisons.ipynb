{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8b292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment                                               Text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"all-data.csv\"  # Update if the file is in a different location\n",
    "data = pd.read_csv(file_path, encoding=\"latin1\", header=None)\n",
    "\n",
    "# Assign meaningful column names\n",
    "data.columns = [\"Sentiment\", \"Text\"]\n",
    "\n",
    "# Check the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ff3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4840 entries, 0 to 4845\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Sentiment  4840 non-null   object\n",
      " 1   Text       4840 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 113.4+ KB\n",
      "None\n",
      "Sentiment\n",
      "neutral     2873\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates and null values\n",
    "data = data.drop_duplicates().dropna()\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(data.info())\n",
    "\n",
    "# Display a summary of sentiment distribution\n",
    "print(data[\"Sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe6edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment                                               Text  \\\n",
      "0   neutral  According to Gran , the company has no plans t...   \n",
      "1   neutral  Technopolis plans to develop in stages an area...   \n",
      "2  negative  The international electronic industry company ...   \n",
      "3  positive  With the new production plant the company woul...   \n",
      "4  positive  According to the company 's updated strategy f...   \n",
      "\n",
      "                                      Processed_Text  \n",
      "0  according gran company plans production russia...  \n",
      "1  technopolis plans develop stages area 100,000 ...  \n",
      "2  international electronic industry company elco...  \n",
      "3  new production plant company increase capacity...  \n",
      "4  according company updated strategy years 2009 ...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocessing function with spaCy\n",
    "def preprocess_text_spacy(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Remove stopwords, punctuation, and convert to lowercase\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Join tokens back into a single string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply the preprocessing function\n",
    "data[\"Processed_Text\"] = data[\"Text\"].apply(preprocess_text_spacy)\n",
    "\n",
    "# Display the first few rows of processed data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "data[\"Sentiment_Label\"] = data[\"Sentiment\"].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d257c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[\"Processed_Text\"]\n",
    "y = data[\"Sentiment_Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446ffc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46799ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.38      0.51       118\n",
      "     neutral       0.73      0.95      0.83       563\n",
      "    positive       0.76      0.46      0.57       287\n",
      "\n",
      "    accuracy                           0.74       968\n",
      "   macro avg       0.75      0.60      0.63       968\n",
      "weighted avg       0.74      0.74      0.71       968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9016e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 0.6042 - loss: 0.9205 - val_accuracy: 0.6606 - val_loss: 0.7821\n",
      "Epoch 2/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 134ms/step - accuracy: 0.7478 - loss: 0.5989 - val_accuracy: 0.7316 - val_loss: 0.6840\n",
      "Epoch 3/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 170ms/step - accuracy: 0.9136 - loss: 0.2683 - val_accuracy: 0.7071 - val_loss: 0.8079\n",
      "Epoch 4/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9587 - loss: 0.1349 - val_accuracy: 0.7123 - val_loss: 0.9720\n",
      "Epoch 5/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 107ms/step - accuracy: 0.9805 - loss: 0.0705 - val_accuracy: 0.6968 - val_loss: 1.1270\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "BiLSTM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.51      0.51       118\n",
      "     neutral       0.76      0.81      0.78       563\n",
      "    positive       0.63      0.55      0.59       287\n",
      "\n",
      "    accuracy                           0.70       968\n",
      "   macro avg       0.64      0.62      0.63       968\n",
      "weighted avg       0.69      0.70      0.69       968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Tokenise and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100  # Maximum sequence length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Step 2: Build BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dense(3, activation=\"softmax\"))  # 3 classes: Positive, Neutral, Negative\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Step 3: Train the model\n",
    "model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Step 4: Evaluate model\n",
    "y_pred_dl = model.predict(X_test_pad).argmax(axis=1)\n",
    "\n",
    "# Step 5: Classification report with correct class names\n",
    "# Define label mapping\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Invert the label mapping to get class names\n",
    "target_names = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Print classification report\n",
    "print(\"BiLSTM Performance:\")\n",
    "print(classification_report(y_test, y_pred_dl, target_names=[target_names[i] for i in sorted(target_names)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a1c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f986792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "file_path = \"all-data.csv\"  # Update this with the correct path to your dataset\n",
    "data = pd.read_csv(file_path, encoding=\"latin1\", header=None)\n",
    "data.columns = [\"Sentiment\", \"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8455194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiment labels to numerical values\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "data[\"Sentiment_Label\"] = data[\"Sentiment\"].map(label_mapping)\n",
    "\n",
    "# Step 2: Split Data\n",
    "X = data[\"Text\"]\n",
    "y = data[\"Sentiment_Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d1cb5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenise Data for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "X_train_encoded = tokenize_data(X_train, tokenizer)\n",
    "X_test_encoded = tokenize_data(X_test, tokenizer)\n",
    "\n",
    "# Step 4: Load Pretrained BERT Model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd72f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TensorFlow-compatible AdamW optimiser\n",
    "num_train_steps = len(X_train_encoded[\"input_ids\"]) // 16 * 3  # Batch size = 16, epochs = 3\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=5e-5, num_train_steps=num_train_steps, num_warmup_steps=0, weight_decay_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "146137d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the BERT model\n",
    "bert_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "bert_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b36b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\ojaga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ojaga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "194/194 [==============================] - 3284s 17s/step - loss: 1.2804 - accuracy: 0.5403 - val_loss: 1.0986 - val_accuracy: 0.2603\n",
      "Epoch 2/3\n",
      "194/194 [==============================] - 3118s 16s/step - loss: 1.0986 - accuracy: 0.4084 - val_loss: 1.0986 - val_accuracy: 0.4046\n",
      "Epoch 3/3\n",
      "194/194 [==============================] - 3125s 16s/step - loss: 1.0986 - accuracy: 0.4219 - val_loss: 1.0986 - val_accuracy: 0.4034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x274bd0aacc0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "bert_model.fit(\n",
    "    {\"input_ids\": X_train_encoded[\"input_ids\"], \"attention_mask\": X_train_encoded[\"attention_mask\"]},\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "820397c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 353s 11s/step\n",
      "BERT Sentiment Analysis Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       110\n",
      "     neutral       0.50      0.51      0.51       571\n",
      "    positive       0.23      0.30      0.26       289\n",
      "\n",
      "    accuracy                           0.39       970\n",
      "   macro avg       0.24      0.27      0.26       970\n",
      "weighted avg       0.36      0.39      0.38       970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ojaga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ojaga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ojaga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the Model\n",
    "y_pred = np.argmax(\n",
    "    bert_model.predict({\"input_ids\": X_test_encoded[\"input_ids\"], \"attention_mask\": X_test_encoded[\"attention_mask\"]}).logits,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Step 8: Classification Report\n",
    "print(\"BERT Sentiment Analysis Performance:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7f514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
